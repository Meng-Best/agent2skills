---
name: read_paper
description: |
  辅助翻译英文学术论文，支持本地 PDF、arXiv、DOI 等来源。
  提供学术严谨的中文翻译，包含术语对照、摘要生成和深度分析。
version: 1.0.0
tags: [translation, paper, academic, AI, ML]

parameters:
  - name: source
    type: string
    required: true
    description: |
      论文来源，支持以下格式：
      - 本地路径: D:\papers\example.pdf
      - arXiv ID: arxiv:2301.00001
      - arXiv URL: https://arxiv.org/abs/2301.00001
      - DOI: doi:10.1234/example

returns:
  type: object
  description: |
    返回翻译结果对象：
    - output_file: 生成的翻译文件路径
    - title: 论文标题（中英文）
    - sections_translated: 翻译的章节数
    - terms_extracted: 提取的术语数量
---

# read_paper Skill - 英文论文翻译助手

## 功能概述

`read_paper` 是一个专为 AI/ML 领域研究者设计的论文翻译工具。它不仅提供高质量的学术翻译，还会在文末给出深度分析，帮助你快速判断论文的价值和创新点。

## 使用方式

```bash
# 本地 PDF 文件
/read_paper D:\papers\attention_is_all_you_need.pdf

# arXiv 论文（ID 格式）
/read_paper arxiv:2301.00001

# arXiv 论文（URL 格式）
/read_paper https://arxiv.org/abs/2301.00001

# DOI 链接
/read_paper doi:10.1038/s41586-021-03819-2
```

## 工作流程

```
┌─────────────────────────────────────────────────────────────┐
│  1. 解析输入 → 识别来源类型（本地/arXiv/DOI）                │
├─────────────────────────────────────────────────────────────┤
│  2. 获取论文                                                 │
│     ├─ 本地 PDF: 直接读取                                   │
│     ├─ arXiv: 抓取 PDF 或 HTML                              │
│     └─ DOI: 尝试开放获取渠道，失败则提示提供本地文件         │
├─────────────────────────────────────────────────────────────┤
│  3. 解析内容                                                 │
│     ├─ 优先: Claude 原生 PDF 视觉能力                       │
│     └─ 回退: Python 库 (pdfplumber/PyMuPDF)                 │
├─────────────────────────────────────────────────────────────┤
│  4. 逐段翻译                                                 │
│     ├─ 风格: 学术严谨，符合中文表达习惯                      │
│     ├─ 术语: 首次出现标注中英对照                            │
│     ├─ 跳过: 参考文献、图片上的文字                          │
│     └─ 保留: 数学公式 (LaTeX 格式)                          │
├─────────────────────────────────────────────────────────────┤
│  5. 二次润色 → 确保译文流畅自然                              │
├─────────────────────────────────────────────────────────────┤
│  6. 生成附加内容                                             │
│     ├─ 论文摘要（要点提炼）                                  │
│     ├─ 术语对照表                                            │
│     └─ 深度分析（创新点、利弊、价值评估）                    │
├─────────────────────────────────────────────────────────────┤
│  7. 输出 Markdown 文件 → 原文件名_翻译.md                    │
└─────────────────────────────────────────────────────────────┘
```

## 输出格式

翻译结果将保存为 Markdown 文件，结构如下：

```markdown
# 论文标题（中文）
> 原标题：Paper Title in English

## 论文摘要
[要点提炼，3-5 个核心观点]

---

## Abstract / 摘要

> [英文原文]

[中文翻译]

## 1. Introduction / 引言

> [英文原文段落]

[中文翻译段落]

...

---

## 术语表

| 英文术语 | 中文翻译 |
|----------|----------|
| Transformer | 变换器 |
| Self-Attention | 自注意力机制 |

---

## 论文深度分析

### 核心创新点
[阐明本文的主要贡献和创新之处]

### 研究现状对比
[与当前 AI/ML 领域相关工作的对比分析]

### 优势与局限

**优势：**
- ...

**局限性：**
- ...

### 可行性评估
[从工程实现和实际应用角度分析]

### 未来价值与追踪建议

**推荐追踪程度：** ⭐⭐⭐⭐☆

[是否值得持续关注，理由是什么]
```

## 翻译规范

### 术语处理

专业术语首次出现时采用中英对照格式：

```
卷积神经网络 (Convolutional Neural Network, CNN) 是一种...
```

后续出现可直接使用中文或英文缩写。

### 特殊内容处理

| 内容类型 | 处理方式 |
|----------|----------|
| 数学公式 | 保留，转为 LaTeX 格式 `$...$` 或 `$$...$$` |
| 表格 | 翻译表格内容 |
| 图片文字 | 不翻译，保持原样 |
| 代码块 | 保留原样，仅翻译注释 |
| 参考文献 | 跳过，不翻译 |

### 翻译风格

- **学术严谨**：保持论文的正式语体
- **中文习惯**：符合中文学术论文的表达方式，避免翻译腔
- **重点标注**：关键结论、创新点使用 **加粗** 标记

## 错误处理

| 场景 | 处理方式 |
|------|----------|
| PDF 解析失败 | 尝试 Python 库回退，仍失败则提示用户 |
| DOI 无法获取 | 尝试 Unpaywall 等开放渠道，失败则要求本地文件 |
| 部分内容无法解析 | 标注 `[解析失败]`，继续翻译其余部分 |
| 非英文论文 | 提示此 skill 专为英文论文设计 |

## 输出位置

- **本地 PDF**：与原文件同目录
- **在线来源**：当前工作目录

文件命名：`原文件名_翻译.md`

## 适用领域

主要针对 **人工智能 / 机器学习** 领域，包括：

- 深度学习 (Deep Learning)
- 自然语言处理 (NLP)
- 计算机视觉 (CV)
- 强化学习 (RL)
- 大语言模型 (LLM)
- 多模态学习 (Multimodal)

## 注意事项

> [!NOTE]
> - 论文翻译可能需要较长时间，请耐心等待
> - 建议优先使用本地 PDF 以确保内容完整性
> - DOI 链接可能因付费墙导致无法获取全文

## 示例

### 输入
```bash
/read_paper arxiv:1706.03762
```

### 输出
生成文件：`1706.03762_翻译.md`

内容包含《Attention Is All You Need》的完整中文翻译、术语表，以及对 Transformer 架构的深度分析。
